# 逻辑回归模型

## 基本形式

给定包含$$m$$条记录的数据集$$D$$：

$$
D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}
$$
线性回归模型试图学习一个线性模型以尽可能地预测因变量$$y$$：

$$
f(x)=w_1x_1+w_2x_2+...+w_px_p+b
$$
## 多元线性回归的假设

同大多数算法一样，多元线性回归的准确性也基于它的假设，在符合假设的情况下构建模型才能得到拟合效果较好的表达式和统计性质较优的估计参数。

* 误差项$$\epsilon$$是一个期望值为零的随机变量，即$$E(\epsilon)=0$$
* $$\epsilon$$的方差是相同的，即$$\sigma^2=VAR(\epsilon)$$
* $$\epsilon$$的值是相互独立的
* $$\epsilon$$是一个服从正态分布的随机变量

## 参数估计

将线性表达式写为向量形式：

$$
f(x)=w^Tx+b
$$
利用最小二乘法令均方误差最小化：

$$
\hat{w}^*=\min_{\hat{w}}(y-X\hat{w})^T(y-X\hat{w})
$$

$$
\hat{w}^*=(X^TX)^{-1}X^Ty
$$
> 注：当线性回归模型存在多重共线性问题时，可能会有多组解使得均方误差最小化，常见的解决方法是引入正则化。

## 线性回归模型的变形
#### 1.对数线性回归
对数线性回归本质上仍然是线性回归模型，只是我们将因变量的对数作为模型新的因变量：

$$
ln y=w^Tx+b
$$
#### 2.广义线性模型
当数据集不适合用传统的多元线性回归方法拟合时，我们可以考虑对因变量做一些合理的变换。最常用的就是对数线性回归，还有很多其他的变换统称为“广义线性模型”`generalized linear model`：

$$
y=g^{-1}(w^Tx+b)
$$
其中$$g(\cdot)$$是单调可微函数。

## 显著性检验
在一元线性回归中，我们可以根据因变量和因变量的图像来检验是否符合线性关系。在多元线性回归中无法用图形帮助判断$$E(Y)$$是否随$$X_1,X_2,...,X_p$$作线性变化，因此显著性检验就显得尤为重要。检验包括单个/多个回归系数的显著性检验和回归方程的整体显著性检验。
#### 1.回归系数的显著性检验
对于任意一个参数$$\beta_i$$，构造原假设与备择假设：
$$
H_0:\beta_i=0;H_1:\beta_i\neq 0
$$
当$$H_0$$成立时，我们构造$$t$$统计量：
$$
T_j=\frac{\hat\beta_j}{\hat\sigma \sqrt{c_{jj}}} \sim t(n-p-1)
$$
其中$$c_{jj}$$是$$C(X^TX)^{-1}$$的对角线上第$$j$$个元素。给定显著性水平$$\alpha$$，检验的拒绝域为：
$$
|T_j|\geq t_{\alpha/2}(n-p-1)
$$
#### 2.回归方程的显著性检验
构造原假设：
$$
H_0:\beta_0=\beta_1=...=\beta_p=0
$$
备择假设即$$\beta_i$$不全为零，当原假设成立时，构造$$F$$统计量：
$$
F=\frac{MSR}{MSE}=\frac{SSR/p}{SSE/(n-p-1)}\sim F(p,n-p-1)
$$
其中$$SSR=\sum_{i=1}^{n}(\hat y_i - \bar y)^2,SSE=\sum_{i=1}^{n}(y_i-\hat y_i)^2$$，通常我们将前者称为回归平方和，后者称为残差平方和。给定显著性水平$$\alpha$$，检验的拒绝域为：
$$
F > F_{\alpha}(p,n-p-1)
$$

> 我们常使用$$R^2=\frac{SSR}{SST}$$来衡量回归直线对观测值的拟合程度，$$SST=\sum_{i=1}^{n}(y_i-\bar y)^2$$表示总体利差平方和，这个思想和回归方程的整体显著性检验殊途同归。

## 参数区间估计

由$$\beta$$的统计性质可知：
$$
T_i=\frac{\beta_i - \beta}{sd(\hat \beta_i)} \sim t(n-p-1)
$$
因此$$\beta_i$$的区间估计可写为：
$$
\Big[ \hat \beta_i - sd(\hat \beta_i)t_{\alpha /2(n-p-1)},  \hat \beta_i + sd(\hat \beta_i)t_{\alpha /2(n-p-1)} \Big]
$$
