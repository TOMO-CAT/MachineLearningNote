# 线性回归模型

## 基本形式
给定包含$$m$$条记录的数据集$$D$$：

$$
D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}
$$
线性回归模型试图学习一个线性模型以尽可能地预测因变量$$y$$：

$$
f(x)=w_1x_1+w_2x_2+...+w_dx_d+b
$$
## 参数估计
将线性表达式写为向量形式：

$$
f(x)=w^Tx+b
$$
利用最小二乘法令均方误差最小化：

$$
\hat{w}^*=\min_{\hat{w}}(y-X\hat{w})^T(y-X\hat{w})
$$

$$
\hat{w}^*=(X^TX)^{-1}X^Ty
$$
> 注：当线性回归模型存在多重共线性问题时，可能会有多组解使得均方误差最小化，常见的解决方法是引入正则化。

## 线性回归模型的变形
#### 1. 对数线性回归
对数线性回归本质上仍然是线性回归模型，只是我们将因变量的对数作为模型新的因变量：

$$
ln y=w^Tx+b
$$
#### 2. 广义线性模型
当数据集不适合用传统的多元线性回归方法拟合时，我们可以考虑对因变量做一些合理的变换。最常用的就是对数线性回归，还有很多其他的变换统称为“广义线性模型”`generalized linear model`：

$$
y=g^{-1}(w^Tx+b)
$$
其中$$g(\cdot)$$是单调可微函数。