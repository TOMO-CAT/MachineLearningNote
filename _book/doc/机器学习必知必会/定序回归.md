# 定序回归

## 简介
传统的线性回归模型预测的因变量取值范围为任意实数，在实际应用中我们常常需要对非连续型数据建模，其中一类的典型的数据即是定序数据`ordinal data`。

> 一般我们以没有数值意义但是有顺序意义的数据统称为定序数据。最常见的例子就是问卷调查给出的选项：非常满意、满意、一般、不满意、非常不满意就是一类定序数据。定序变量介于连续变量和定类变量之间，是在测量层次上被分为相对次序的不同类别、但并不连续。

## 使用定序回归的原因
如果对定序变量使用多分类`logit`模型，那么会无视数据内在的排序从而导致排序信息的缺失，使得统计结果因为遗漏掉排序信息而丧失统计效率。如果使用普通线性回归模型，那么就是将定序变量视为连续变量处理，会导致人为的信息膨胀。因此，针对定序变量，需要采用对应的模型来拟合其两方面的性质，最常用的方法即定序回归模型`ordered logit/probit model`。

## 建立模型
当我们评价某产品时，会形成对一个产品的喜好程度，我们将其记为$$Z$$，它反映了个人对其的标准化评分。而要把对产品的喜好程度转化为问卷上的打分时需要建立一定的判断标准，这一套判断标准由一组阈值$$c_k$$构成，喜好落在某两个相邻的阈值之间就会给出某个具体的打分（非常满意、满意、一般、不满意、非常不满意中的一个）。具体的对应方式如下所示：
$$
scores =
\begin{cases}
1, Z < c_1 \\
2, c_1 \leq Z < c_2 \\
3, c_2 \leq Z < c_3 \\
4, c_3 \leq Z < c_4 \\
5, c_4 \leq Z
\end{cases}
$$
我们假设因变量是通过影响喜好程度来影响消费者打分$$scores = 1,2,...,5$$，而 $$Z$$ 是一个取任意值的连续性变量。我们可以用普通线性回归模型来刻画$$Z$$ 与因变量$$W$$之间的关系：
$$
Z=\beta_0 + \beta * W + \epsilon
$$
从而判断分数不超过$$k$$的概率就是：
$$
\begin{aligned}
P(scores < k) &= P(Z \leq c_k) \\
&= P(\beta_0 + \beta * W + \epsilon \leq c_k) \\
&= P(\epsilon \leq c_k -\beta_0 - \beta * W) \\
&= F_{\epsilon}(\alpha_k - \beta * W)
\end{aligned}
$$
其中$$\alpha_k = c_k -\beta_0$$，$$F_{\epsilon}(t) = P(\epsilon < t)$$是$$\epsilon$$的分布函数。至此我们获得了一个关于定序变量的回归模型，如下所示：
$$
P(scores \leq k) = F_{\epsilon}(\alpha_k - \beta * W)
$$

## probit定序回归和logit定序回归

由于定序数据可能取值的数量大于$$2$$（相较于普通的零一回归），因此会有好几个不同的截距$$\alpha_k$$。例如当`scores`有$$5$$个不同的取值时会对应$$4$$个不同的截距项，并且截距之间满足关系$$\alpha_1 < \alpha_2 < \alpha_3 < \alpha_4$$，斜率$$\beta$$只有一个取值。在实际问题建模中，我们更关心因变量和自变量的相互关系而非截距项的取值。  
有一个方便的假定是将$$F_{\epsilon}(t)$$假设成标准正态分布和逻辑分布，分别对应着`probit`定序回归和`logit`定序回归。模型分别如下：
$$
P(score \leq k) = \Phi(\alpha_k - \beta * W)
$$
$$
P(score \leq k) = \frac{exp(\alpha_k - \beta * W)}{1+ exp(\alpha_k - \beta * W)}
$$

> `probit`定序回归和`logit`定序回归之间孰优孰劣至今没有定论，但是都是非常有用的统计方法，并且统计结果往往及其相似。

## 参数估计与统计推断
> 由于`probit`和`logit`定序回归的估计方法和推断方法极其相似，因此我们中讨论`probit`定序回归。

假设$$score_i$$和$$W_i$$来自第$$i$$个样本的观测，则样本取值为$$k$$的概率为：
$$
P(score = k) = f_k(W_i)
= \begin{cases}
\Phi(\alpha_1 - \beta * W_i), k=1 \\
\Phi(\alpha_k - \beta * W_i)-\Phi(\alpha_{k-1} - \beta *W_i), 1<k<5 \\
1-\Phi(\alpha_4 - \beta * w_i), k=5
\end{cases}
$$
样本似然函数为：
$$
L(\beta_0, \beta) = \prod_{i=1}^{n}\prod_{k=1}^{5}\{f_k(W_i)\}^{I\{score_i = k\}}
$$
> 一般对于指数级/连乘函数求极值时常使用取对数的方法，方便求导降低计算量。

对应的极大似然函数为：
$$
log(L(\beta_0, \beta)) = \sum_{i=1}^{n} \sum_{k=1}^{5}I\{score_i = k\} * log\{f_k{W_i}\}
$$
求解极大似然函数我们可以获得参数估计值为$$(\hat{\beta_0}, \hat{\beta})$$，我们并不知道$$(\hat{\beta_0}, \hat{\beta})$$的具体分布，但是在样本量足够大的情况下，根据中心极限定理：
$$
\frac{\hat{\beta_j}-\beta_j}{\sqrt{Var(\hat{\beta_j})}} = \frac{\hat{\beta_j}-\beta_j}{\sigma(\hat{\beta_j})} \sim N(0, 1)
$$
我们可以构造检验统计量$$T_j=\frac{\hat{\beta_j}}{\hat{\sigma(\hat{\beta_j})}}$$，当原假设$$\beta_j=0$$成立时，该统计量近似服从标准正态分布。因此可以根据$$T_j$$的绝对值是否大于$$z_{1-\alpha/2}$$来判断是否拒绝原假设。

> $$\alpha$$指的是显著性水平，即参数落在某一区间内可能犯错的概率。

